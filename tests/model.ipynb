{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, sigmoid\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Masked sequence</th>\n",
       "      <th>Y1</th>\n",
       "      <th>Y2</th>\n",
       "      <th>Count target</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[False, False, False, True, False], [False, F...</td>\n",
       "      <td>False</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[True, False, False, False, False], [False, F...</td>\n",
       "      <td>True</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>1</td>\n",
       "      <td>[85816787]</td>\n",
       "      <td>[85816895]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[False, False, False, True, False], [False, F...</td>\n",
       "      <td>False</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[True, False, False, False, False], [False, T...</td>\n",
       "      <td>False</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[True, False, False, False, False], [False, F...</td>\n",
       "      <td>True</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>1</td>\n",
       "      <td>[183577598]</td>\n",
       "      <td>[183577707]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Masked sequence     Y1  \\\n",
       "0  [[False, False, False, True, False], [False, F...  False   \n",
       "1  [[True, False, False, False, False], [False, F...   True   \n",
       "2  [[False, False, False, True, False], [False, F...  False   \n",
       "3  [[True, False, False, False, False], [False, T...  False   \n",
       "4  [[True, False, False, False, False], [False, F...   True   \n",
       "\n",
       "                                                  Y2  Count target  \\\n",
       "0  [False, False, False, False, False, False, Fal...             0   \n",
       "1  [False, False, False, False, False, False, Fal...             1   \n",
       "2  [False, False, False, False, False, False, Fal...             0   \n",
       "3  [False, False, False, False, False, False, Fal...             0   \n",
       "4  [False, False, False, False, False, False, Fal...             1   \n",
       "\n",
       "         Start          End  \n",
       "0         None         None  \n",
       "1   [85816787]   [85816895]  \n",
       "2         None         None  \n",
       "3         None         None  \n",
       "4  [183577598]  [183577707]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('F:\\projects\\dna_ai\\data\\data.pkl')\n",
    "df = df.sample(frac = 1, ignore_index=True) # перемешаем\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False, False],\n",
       "       [False, False, False,  True, False],\n",
       "       [ True, False, False, False, False],\n",
       "       ...,\n",
       "       [False, False, False,  True, False],\n",
       "       [False, False, False,  True, False],\n",
       "       [False, False, False,  True, False]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Masked sequence'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Masked sequence'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = df['Masked sequence'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False,  True, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False,  True, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Masked sequence'][1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(df['Masked sequence'][1], (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(df['Masked sequence'][1], (m, n)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39372, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность тренировочной выборки: (23622,)\n",
      "Размерность тестовой выборки: (7875,)\n",
      "Размерность маски теста: (1000,)\n",
      "Размерность валидации при обучении: (7875,)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Разделение на тренировочную и тестовую выборки\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df['Masked sequence'], df['Y2'], test_size=0.2)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25)\n",
    "\n",
    "# # Вывод размерностей тренировочной и тестовой выбороки\n",
    "# print(\"Размерность тренировочной выборки:\", x_train.shape)\n",
    "# print(\"Размерность тестовой выборки:\", x_test.shape)\n",
    "# print(\"Размерность маски теста:\", y_test.iloc[0].shape)\n",
    "# print(\"Размерность валидации при обучении:\", x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():     # Make sure GPU is available\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"Warning: CUDA not found, CPU only.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.df = pd.read_pickle(path)\n",
    "        self.df = df.sample(frac = 1, ignore_index=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.df['Masked sequence'][index].T.astype(float)\n",
    "        y = self.df['Y2'][index].astype(float)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SequenceDataset('F:\\projects\\dna_ai\\data\\data.pkl')\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39372"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31497 7875\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_size = int(0.8 * len(data))\n",
    "valid_size = len(data) - train_size\n",
    "print(train_size, valid_size)\n",
    "\n",
    "train_dataset, valid_dataset = random_split(data, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.Subset at 0x152ada992a0>,\n",
       " <torch.utils.data.dataset.Subset at 0x152ada98d90>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 62)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.DoubleTensor'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x, y = next(iter(train_loader))\n",
    "# x.type()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaumanNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaumanNet, self).__init__()\n",
    "        \n",
    "        self.conv11 = nn.Conv1d(in_channels=5, out_channels=16, kernel_size=1, padding= 'same')\n",
    "        #2d change\n",
    "        \"Encoder\"\n",
    "        self.conv12 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv21 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv22 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv31 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv32 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        \"Bottle neck\"\n",
    "\n",
    "        self.conv41 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv42 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        \n",
    "        \"Decoder\"\n",
    "        self.upconv5 = nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
    "        self.conv51 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv52 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv6 = nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=2, stride=2)\n",
    "        self.conv61 = nn.Conv1d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv62 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv7 = nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=2, stride=2)\n",
    "        self.conv71 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv72 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.output = nn.Conv1d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        xe11 = relu(self.conv11(x))\n",
    "        \"Encoder\"\n",
    "        xe12 = relu(self.conv12(xe11))\n",
    "        xe13 = self.maxpool1(xe12)\n",
    "\n",
    "        xe21 = relu(self.conv21(xe13))\n",
    "        xe22 = relu(self.conv22(xe21))\n",
    "        xe23 = self.maxpool2(xe22)\n",
    "\n",
    "        xe31 = relu(self.conv31(xe23))\n",
    "        xe32 = relu(self.conv32(xe31))\n",
    "        xe33 = self.maxpool3(xe32)\n",
    "\n",
    "        \"Bottle neck\"\n",
    "        xe41 = relu(self.conv41(xe33))\n",
    "        xe42 = relu(self.conv42(xe41))\n",
    "\n",
    "        \"Decoder\"\n",
    "        xe51 = self.upconv5(xe42)\n",
    "        xe5 = torch.cat([xe51, xe32], dim=1)\n",
    "        xe52 = relu(self.conv51(xe5))\n",
    "        xe53 = relu(self.conv52(xe52))\n",
    "\n",
    "        xe61 = self.upconv6(xe53)\n",
    "        xe6 = torch.cat([xe61, xe22], dim=1)\n",
    "        xe62 = relu(self.conv61(xe6))\n",
    "        xe63 = relu(self.conv62(xe62))\n",
    "\n",
    "        xe71 = self.upconv7(xe63)\n",
    "        xe7 = torch.cat([xe71, xe12], dim=1)\n",
    "        xe72 = relu(self.conv71(xe7))\n",
    "        xe73 = relu(self.conv72(xe72))\n",
    "\n",
    "    \n",
    "        out = sigmoid(self.output(xe73))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv1d: 1-1                            [-1, 16, 1000]            96\n",
      "├─Conv1d: 1-2                            [-1, 16, 1000]            784\n",
      "├─MaxPool1d: 1-3                         [-1, 16, 500]             --\n",
      "├─Conv1d: 1-4                            [-1, 32, 500]             1,568\n",
      "├─Conv1d: 1-5                            [-1, 32, 500]             3,104\n",
      "├─MaxPool1d: 1-6                         [-1, 32, 250]             --\n",
      "├─Conv1d: 1-7                            [-1, 64, 250]             6,208\n",
      "├─Conv1d: 1-8                            [-1, 64, 250]             12,352\n",
      "├─MaxPool1d: 1-9                         [-1, 64, 125]             --\n",
      "├─Conv1d: 1-10                           [-1, 128, 125]            24,704\n",
      "├─Conv1d: 1-11                           [-1, 128, 125]            49,280\n",
      "├─ConvTranspose1d: 1-12                  [-1, 64, 250]             16,448\n",
      "├─Conv1d: 1-13                           [-1, 64, 250]             24,640\n",
      "├─Conv1d: 1-14                           [-1, 64, 250]             12,352\n",
      "├─ConvTranspose1d: 1-15                  [-1, 32, 500]             4,128\n",
      "├─Conv1d: 1-16                           [-1, 32, 500]             6,176\n",
      "├─Conv1d: 1-17                           [-1, 32, 500]             3,104\n",
      "├─ConvTranspose1d: 1-18                  [-1, 16, 1000]            1,040\n",
      "├─Conv1d: 1-19                           [-1, 16, 1000]            1,552\n",
      "├─Conv1d: 1-20                           [-1, 16, 1000]            784\n",
      "├─Conv1d: 1-21                           [-1, 1, 1000]             17\n",
      "==========================================================================================\n",
      "Total params: 168,337\n",
      "Trainable params: 168,337\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 40.29\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.08\n",
      "Params size (MB): 0.64\n",
      "Estimated Total Size (MB): 2.74\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv1d: 1-1                            [-1, 16, 1000]            96\n",
       "├─Conv1d: 1-2                            [-1, 16, 1000]            784\n",
       "├─MaxPool1d: 1-3                         [-1, 16, 500]             --\n",
       "├─Conv1d: 1-4                            [-1, 32, 500]             1,568\n",
       "├─Conv1d: 1-5                            [-1, 32, 500]             3,104\n",
       "├─MaxPool1d: 1-6                         [-1, 32, 250]             --\n",
       "├─Conv1d: 1-7                            [-1, 64, 250]             6,208\n",
       "├─Conv1d: 1-8                            [-1, 64, 250]             12,352\n",
       "├─MaxPool1d: 1-9                         [-1, 64, 125]             --\n",
       "├─Conv1d: 1-10                           [-1, 128, 125]            24,704\n",
       "├─Conv1d: 1-11                           [-1, 128, 125]            49,280\n",
       "├─ConvTranspose1d: 1-12                  [-1, 64, 250]             16,448\n",
       "├─Conv1d: 1-13                           [-1, 64, 250]             24,640\n",
       "├─Conv1d: 1-14                           [-1, 64, 250]             12,352\n",
       "├─ConvTranspose1d: 1-15                  [-1, 32, 500]             4,128\n",
       "├─Conv1d: 1-16                           [-1, 32, 500]             6,176\n",
       "├─Conv1d: 1-17                           [-1, 32, 500]             3,104\n",
       "├─ConvTranspose1d: 1-18                  [-1, 16, 1000]            1,040\n",
       "├─Conv1d: 1-19                           [-1, 16, 1000]            1,552\n",
       "├─Conv1d: 1-20                           [-1, 16, 1000]            784\n",
       "├─Conv1d: 1-21                           [-1, 1, 1000]             17\n",
       "==========================================================================================\n",
       "Total params: 168,337\n",
       "Trainable params: 168,337\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 40.29\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 2.08\n",
       "Params size (MB): 0.64\n",
       "Estimated Total Size (MB): 2.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaumanNet().to(device)\n",
    "# x, y = next(iter(train_loader))\n",
    "summary(model, (5, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = DiceLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка 1 эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "LOSS train 0.0 valid 0.8443590998649597\n",
      "EPOCH 2:\n",
      "LOSS train 0.0 valid 0.8443590998649597\n"
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(valid_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs = vinputs.to(device).float()\n",
    "            vlabels = vlabels.to(device).float()\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Encoder\n",
    "            xe11 = relu(self.e11(x))\n",
    "            xe12 = relu(self.e12(xe11))\n",
    "            xp1 = self.pool1(xe12)\n",
    "\n",
    "            xe21 = relu(self.e21(xp1))\n",
    "            xe22 = relu(self.e22(xe21))\n",
    "            xp2 = self.pool2(xe22)\n",
    "\n",
    "            xe31 = relu(self.e31(xp2))\n",
    "            xe32 = relu(self.e32(xe31))\n",
    "            xp3 = self.pool3(xe32)\n",
    "\n",
    "            xe41 = relu(self.e41(xp3))\n",
    "            xe42 = relu(self.e42(xe41))\n",
    "            xp4 = self.pool4(xe42)\n",
    "\n",
    "            xe51 = relu(self.e51(xp4))\n",
    "            xe52 = relu(self.e52(xe51))\n",
    "\n",
    "            # Decoder\n",
    "            xu1 = self.upconv1(xe52)\n",
    "            xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "            xd11 = relu(self.d11(xu11))\n",
    "            xd12 = relu(self.d12(xd11))\n",
    "\n",
    "            xu2 = self.upconv2(xd12)\n",
    "            xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "            xd21 = relu(self.d21(xu22))\n",
    "            xd22 = relu(self.d22(xd21))\n",
    "\n",
    "            xu3 = self.upconv3(xd22)\n",
    "            xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "            xd31 = relu(self.d31(xu33))\n",
    "            xd32 = relu(self.d32(xd31))\n",
    "\n",
    "            xu4 = self.upconv4(xd32)\n",
    "            xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "            xd41 = relu(self.d41(xu44))\n",
    "            xd42 = relu(self.d42(xd41))\n",
    "\n",
    "            # Output layer\n",
    "            out = self.outconv(xd42)\n",
    "\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# выбор функции активацииЖ\n",
    "activation_funk = 'relu'\n",
    "# activation_funk = 'tanh'\n",
    "# activation_funk = 'sigmoid'\n",
    "# activation_funk = leaky_relu_layer\n",
    "\n",
    "conv_dim= [16, 32, 64]\n",
    "conv_size  = [40, 40, 20]\n",
    "maxpool_size = [5, 5, 5]\n",
    "\n",
    "dpout = 0.2\n",
    "\n",
    "\n",
    "\n",
    "metrix = dice1_metric\n",
    "metrix_loss = dice1_loss\n",
    "\n",
    "# metrix = dice_modified\n",
    "# metrix_loss = dice_modified_loss\n",
    "\n",
    "# metrix = dice_symetr1\n",
    "# metrix_loss = dice_symetr1_loss\n",
    "\n",
    "# metrix_loss = dice_cuted_loss\n",
    "# metrix = dice_cuted\n",
    "\n",
    "class Unet(nn.Module):\n",
    "  def __init__(self, input_length, input_channels):\n",
    "      super().__init__()\n",
    "    \n",
    "\n",
    "      self.model = Sequential()\n",
    "      self.input_length = input_length\n",
    "      self.input_channels = input_channels\n",
    "      self.tensorboard_callback = None\n",
    "\n",
    "  def build_model(self):\n",
    "      # Encoder\n",
    "              # Define input tensor\n",
    "      inputs = Input(shape=(self.input_length, self.input_channels))\n",
    "\n",
    "      # Encoder\n",
    "      encoder1 = Conv1D(conv_dim[0], conv_size[0], activation=activation_funk, strides=1, padding='same')(inputs)\n",
    "      # encoder1 = Conv1D(conv_dim[0], conv_size[0], activation=activation_funk, strides=1, padding='same')(encoder1)\n",
    "      # encoder1 = Conv1D(conv_dim[0], conv_size[0], activation=activation_funk, strides=1, padding='same')(encoder1)\n",
    "      encoder1_pool = MaxPooling1D(maxpool_size[0])(encoder1)\n",
    "      encoder1_dropout = Dropout(dpout)(encoder1_pool)\n",
    "\n",
    "      encoder2 = Conv1D(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(encoder1_dropout)\n",
    "      # encoder2 = Conv1D(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(encoder2)\n",
    "      # encoder2 = Conv1D(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(encoder2)\n",
    "      encoder2_pool = MaxPooling1D(maxpool_size[1])(encoder2)\n",
    "\n",
    "      encoder3 = Conv1D(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder2_pool)\n",
    "      # encoder3 = Conv1D(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder3)\n",
    "      # encoder3 = Conv1D(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder3)\n",
    "      encoder3_pool = MaxPooling1D(maxpool_size[2])(encoder3)\n",
    "\n",
    "      # Decoder\n",
    "      decoder1 = Conv1DTranspose(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder3_pool)\n",
    "      # decoder1 = Conv1DTranspose(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(decoder1)\n",
    "      # decoder1 = Conv1DTranspose(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(decoder1)\n",
    "      decoder1_upsample = UpSampling1D(maxpool_size[2])(decoder1)\n",
    "      decoder1_concat = concatenate([decoder1_upsample, encoder3])\n",
    "\n",
    "      decoder2 = Conv1DTranspose(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(decoder1_concat)\n",
    "      # decoder2 = Conv1DTranspose(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(decoder2)\n",
    "      # decoder2 = Conv1DTranspose(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(decoder2)\n",
    "      decoder2_upsample = UpSampling1D(maxpool_size[1])(decoder2)\n",
    "\n",
    "      decoder2_dropout = Dropout(dpout)(decoder2_upsample)\n",
    "      decoder2_concat = concatenate([decoder2_dropout, encoder2])\n",
    "\n",
    "      decoder3 = Conv1DTranspose(conv_dim[0], conv_size[0], activation=activation_funk, padding='same')(decoder2_concat)\n",
    "      # decoder3 = Conv1DTranspose(conv_dim[0], conv_size[0], activation=activation_funk, padding='same')(decoder3)\n",
    "      # decoder3 = Conv1DTranspose(conv_dim[0], conv_size[0], activation=activation_funk, padding='same')(decoder3)\n",
    "      decoder3_upsample = UpSampling1D(maxpool_size[0])(decoder3)\n",
    "\n",
    "      decoder3_concat = concatenate([decoder3_upsample, encoder1])\n",
    "\n",
    "      output = Conv1D(1, 1, activation='sigmoid', padding='same')(decoder3_concat)\n",
    "\n",
    "\n",
    "      # Define model with input and output tensors\n",
    "      self.model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "\n",
    "      # Compile the model with dice loss and metric\n",
    "      self.model.compile(optimizer='adam', loss=metrix_loss, metrics=[metrix])\n",
    "\n",
    "      # # Create a TensorBoard callback\n",
    "      self.tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "\n",
    "\n",
    "  def summary(self):\n",
    "      self.model.summary()\n",
    "\n",
    "  def train(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "\n",
    "      log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      self.model.fit (\n",
    "          x_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorboard_callback]\n",
    "\n",
    "          )\n",
    "\n",
    "\n",
    "# Create an instance of the Autoencoder class\n",
    "autoencoder = Autoencoder(input_length, input_channels)\n",
    "autoencoder.build_model()\n",
    "autoencoder.summary()\n",
    "\n",
    "\n",
    "def forward_pass(model, x):\n",
    "    # Преобразование входных данных в тензор TensorFlow\n",
    "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "    # Выполнение прямого прохода\n",
    "    y_pred = model(x_tensor)\n",
    "\n",
    "    # Преобразование предсказания в массив NumPy\n",
    "    y_pred_np = y_pred.numpy()\n",
    "\n",
    "    return y_pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(x_train, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(y_train, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.summary import create_file_writer\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # Запись логов в тензорборд\n",
    "writer = SummaryWriter() # Создание объекта для записи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv1DTranspose, UpSampling1D, Dropout\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv1DTranspose, UpSampling1D\n",
    "from tensorflow.keras.layers import concatenate, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# выбор функции активацииЖ\n",
    "activation_funk = 'relu'\n",
    "# activation_funk = 'tanh'\n",
    "# activation_funk = 'sigmoid'\n",
    "# activation_funk = leaky_relu_layer\n",
    "\n",
    "conv_dim= [16, 32, 64]\n",
    "conv_size  = [40, 40, 20]\n",
    "maxpool_size = [5, 5, 5]\n",
    "\n",
    "dpout = 0.2\n",
    "\n",
    "\n",
    "\n",
    "metrix = dice1_metric\n",
    "metrix_loss = dice1_loss\n",
    "\n",
    "# metrix = dice_modified\n",
    "# metrix_loss = dice_modified_loss\n",
    "\n",
    "# metrix = dice_symetr1\n",
    "# metrix_loss = dice_symetr1_loss\n",
    "\n",
    "# metrix_loss = dice_cuted_loss\n",
    "# metrix = dice_cuted\n",
    "\n",
    "class Autoencoder:\n",
    "  def __init__(self, input_length, input_channels):\n",
    "      self.model = Sequential()\n",
    "      self.input_length = input_length\n",
    "      self.input_channels = input_channels\n",
    "      self.tensorboard_callback = None\n",
    "\n",
    "  def build_model(self):\n",
    "      # Encoder\n",
    "              # Define input tensor\n",
    "      inputs = Input(shape=(self.input_length, self.input_channels))\n",
    "\n",
    "      # Encoder\n",
    "      encoder1 = Conv1D(conv_dim[0], conv_size[0], activation=activation_funk, strides=1, padding='same')(inputs)\n",
    "      # encoder1 = Conv1D(conv_dim[0], conv_size[0], activation=activation_funk, strides=1, padding='same')(encoder1)\n",
    "      # encoder1 = Conv1D(conv_dim[0], conv_size[0], activation=activation_funk, strides=1, padding='same')(encoder1)\n",
    "      encoder1_pool = MaxPooling1D(maxpool_size[0])(encoder1)\n",
    "      encoder1_dropout = Dropout(dpout)(encoder1_pool)\n",
    "\n",
    "      encoder2 = Conv1D(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(encoder1_dropout)\n",
    "      # encoder2 = Conv1D(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(encoder2)\n",
    "      # encoder2 = Conv1D(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(encoder2)\n",
    "      encoder2_pool = MaxPooling1D(maxpool_size[1])(encoder2)\n",
    "\n",
    "      encoder3 = Conv1D(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder2_pool)\n",
    "      # encoder3 = Conv1D(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder3)\n",
    "      # encoder3 = Conv1D(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder3)\n",
    "      encoder3_pool = MaxPooling1D(maxpool_size[2])(encoder3)\n",
    "\n",
    "      # Decoder\n",
    "      decoder1 = Conv1DTranspose(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(encoder3_pool)\n",
    "      # decoder1 = Conv1DTranspose(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(decoder1)\n",
    "      # decoder1 = Conv1DTranspose(conv_dim[2], conv_size[2], activation=activation_funk, padding='same')(decoder1)\n",
    "      decoder1_upsample = UpSampling1D(maxpool_size[2])(decoder1)\n",
    "      decoder1_concat = concatenate([decoder1_upsample, encoder3])\n",
    "\n",
    "      decoder2 = Conv1DTranspose(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(decoder1_concat)\n",
    "      # decoder2 = Conv1DTranspose(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(decoder2)\n",
    "      # decoder2 = Conv1DTranspose(conv_dim[1], conv_size[1], activation=activation_funk, padding='same')(decoder2)\n",
    "      decoder2_upsample = UpSampling1D(maxpool_size[1])(decoder2)\n",
    "\n",
    "      decoder2_dropout = Dropout(dpout)(decoder2_upsample)\n",
    "      decoder2_concat = concatenate([decoder2_dropout, encoder2])\n",
    "\n",
    "      decoder3 = Conv1DTranspose(conv_dim[0], conv_size[0], activation=activation_funk, padding='same')(decoder2_concat)\n",
    "      # decoder3 = Conv1DTranspose(conv_dim[0], conv_size[0], activation=activation_funk, padding='same')(decoder3)\n",
    "      # decoder3 = Conv1DTranspose(conv_dim[0], conv_size[0], activation=activation_funk, padding='same')(decoder3)\n",
    "      decoder3_upsample = UpSampling1D(maxpool_size[0])(decoder3)\n",
    "\n",
    "      decoder3_concat = concatenate([decoder3_upsample, encoder1])\n",
    "\n",
    "      output = Conv1D(1, 1, activation='sigmoid', padding='same')(decoder3_concat)\n",
    "\n",
    "\n",
    "      # Define model with input and output tensors\n",
    "      self.model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "\n",
    "      # Compile the model with dice loss and metric\n",
    "      self.model.compile(optimizer='adam', loss=metrix_loss, metrics=[metrix])\n",
    "\n",
    "      # # Create a TensorBoard callback\n",
    "      self.tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "\n",
    "\n",
    "  def summary(self):\n",
    "      self.model.summary()\n",
    "\n",
    "  def train(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "\n",
    "      log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "      self.model.fit (\n",
    "          x_train,\n",
    "          y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[tensorboard_callback]\n",
    "\n",
    "          )\n",
    "\n",
    "\n",
    "# Create an instance of the Autoencoder class\n",
    "autoencoder = Autoencoder(input_length, input_channels)\n",
    "autoencoder.build_model()\n",
    "autoencoder.summary()\n",
    "\n",
    "\n",
    "def forward_pass(model, x):\n",
    "    # Преобразование входных данных в тензор TensorFlow\n",
    "    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "    # Выполнение прямого прохода\n",
    "    y_pred = model(x_tensor)\n",
    "\n",
    "    # Преобразование предсказания в массив NumPy\n",
    "    y_pred_np = y_pred.numpy()\n",
    "\n",
    "    return y_pred_np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
